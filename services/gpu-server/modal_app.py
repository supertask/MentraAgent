"""
Modal GPU Server Application
WhisperXã€pyannoteã€Vision LLMã‚’ä½¿ç”¨ã—ãŸé«˜åº¦ãªAIå‡¦ç†
"""

import modal
import os
from typing import Dict, List, Any, Optional
from datetime import datetime

# Modalã‚¢ãƒ—ãƒªã®ä½œæˆ
app = modal.App("realworld-agent")

# GPUç’°å¢ƒã¨ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”¨ã®Volume
models_volume = modal.Volume.from_name("realworld-agent-models", create_if_missing=True)

# Docker ã‚¤ãƒ¡ãƒ¼ã‚¸ã®å®šç¾©
image = (
    modal.Image.debian_slim(python_version="3.11")
    .apt_install(
        "ffmpeg",
        "git",
        "pkg-config",
        "libavformat-dev",
        "libavcodec-dev",
        "libavdevice-dev",
        "libavutil-dev",
        "libswscale-dev",
        "libswresample-dev",
        "libavfilter-dev",
    )
    .pip_install(
        # ä»•æ§˜æ›¸ç”Ÿæˆã«å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ï¼ˆæœ€å°æ§‹æˆï¼‰
        "anthropic>=0.40.0",
        "openai>=1.54.0",
        "Pillow>=10.2.0",
        "python-dotenv>=1.0.1",
        "fastapi[all]>=0.109.2",
        "pydantic>=2.6.1",
        # éŸ³å£°æ–‡å­—èµ·ã“ã—ç”¨ï¼ˆç¾åœ¨ã¯ç„¡åŠ¹åŒ–ï¼‰
        # "torch==2.1.0",
        # "torchaudio==2.1.0",
        # "whisperx==3.1.1",
        # "pyannote.audio==3.1.1",
        # "openai-whisper==20231117",
        # "faster-whisper==0.9.0",
        # "numpy==1.24.3",
        # "scipy==1.11.4",
        # "scikit-learn==1.3.2",
        # "opencv-python-headless==4.8.1.78",
    )
)

# Secretsï¼ˆç’°å¢ƒå¤‰æ•°ï¼‰
secrets = [
    modal.Secret.from_name("realworld-agent-secrets"),
]


@app.cls(
    image=image,
    # gpu="A10G",  # GPUã¯éŸ³å£°æ–‡å­—èµ·ã“ã—æ™‚ã®ã¿å¿…è¦ï¼ˆç¾åœ¨ã¯ç„¡åŠ¹åŒ–ï¼‰
    secrets=secrets,
    volumes={"/models": models_volume},
    timeout=600,  # 10åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
    # keep_warm=1,  # ã‚³ã‚¹ãƒˆå‰Šæ¸›ã®ãŸã‚ç„¡åŠ¹åŒ–ï¼ˆã‚³ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ¼ãƒˆã‚ã‚Šï¼š20-60ç§’ï¼‰
)
class RealworldAgentGPU:
    """
    GPUã‚’ä½¿ç”¨ã—ãŸAIå‡¦ç†ã‚¯ãƒ©ã‚¹
    """

    def __init__(self):
        """åˆæœŸåŒ–"""
        self.whisperx_model = None
        self.diarization_pipeline = None
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.anthropic_api_key = os.getenv("ANTHROPIC_API_KEY")
        self.primary_llm_provider = os.getenv("PRIMARY_LLM_PROVIDER", "openai")
        self.enable_llm_fallback = os.getenv("ENABLE_LLM_FALLBACK", "true").lower() == "true"
        self.openai_model = os.getenv("OPENAI_MODEL", "gpt-4o")
        self.anthropic_model = os.getenv("ANTHROPIC_MODEL", "claude-3-5-sonnet-20241022")

    @modal.enter()
    def load_models(self):
        """ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆèµ·å‹•æ™‚ã«1å›ã ã‘å®Ÿè¡Œï¼‰"""
        print("ğŸš€ åˆæœŸåŒ–ä¸­...")
        print("âœ… åˆæœŸåŒ–å®Œäº†")
        print("â„¹ï¸  WhisperX/pyannoteã¯ç¾åœ¨ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™ï¼ˆä»•æ§˜æ›¸ç”Ÿæˆã®ã¿åˆ©ç”¨å¯èƒ½ï¼‰")

    @modal.method()
    def transcribe_audio(
        self,
        audio_path: str,
        language: str = "ja",
        enable_diarization: bool = True,
        min_speakers: int = 1,
        max_speakers: int = 10,
    ) -> Dict[str, Any]:
        """
        éŸ³å£°æ–‡å­—èµ·ã“ã—ï¼ˆWhisperX + è©±è€…åˆ†é›¢ï¼‰

        Args:
            audio_path: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            language: è¨€èªã‚³ãƒ¼ãƒ‰
            enable_diarization: è©±è€…åˆ†é›¢ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹
            min_speakers: æœ€å°è©±è€…æ•°
            max_speakers: æœ€å¤§è©±è€…æ•°

        Returns:
            æ–‡å­—èµ·ã“ã—çµæœ
        """
        import whisperx

        print(f"ğŸ™ï¸ éŸ³å£°æ–‡å­—èµ·ã“ã—é–‹å§‹: {audio_path}")

        try:
            # éŸ³å£°èª­ã¿è¾¼ã¿
            audio = whisperx.load_audio(audio_path)

            # WhisperXã§æ–‡å­—èµ·ã“ã—
            print("ğŸ“ WhisperXã§æ–‡å­—èµ·ã“ã—ä¸­...")
            result = self.whisperx_model.transcribe(
                audio,
                language=language,
                batch_size=16,
            )

            # å˜èªãƒ¬ãƒ™ãƒ«ã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ
            print("ğŸ”¤ å˜èªãƒ¬ãƒ™ãƒ«ã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆä¸­...")
            model_a, metadata = whisperx.load_align_model(
                language_code=language,
                device=self.device,
            )
            result = whisperx.align(
                result["segments"],
                model_a,
                metadata,
                audio,
                self.device,
                return_char_alignments=False,
            )

            # è©±è€…åˆ†é›¢
            speakers_result = None
            if enable_diarization and self.diarization_pipeline:
                print("ğŸ¤ è©±è€…åˆ†é›¢ä¸­...")
                try:
                    diarization = self.diarization_pipeline(
                        audio_path,
                        min_speakers=min_speakers,
                        max_speakers=max_speakers,
                    )

                    # è©±è€…æƒ…å ±ã‚’çµæœã«çµ±åˆ
                    result = whisperx.assign_word_speakers(diarization, result)
                    speakers_result = self._extract_speakers(result)
                    print(f"âœ… {len(set(speakers_result))} äººã®è©±è€…ã‚’æ¤œå‡º")
                except Exception as e:
                    print(f"âš ï¸ è©±è€…åˆ†é›¢ã‚¨ãƒ©ãƒ¼: {e}")

            # çµæœã®æ•´å½¢
            output = {
                "text": " ".join([seg["text"] for seg in result["segments"]]),
                "segments": result["segments"],
                "language": language,
                "speakers": speakers_result,
                "timestamp": datetime.now().isoformat(),
            }

            print("âœ… éŸ³å£°æ–‡å­—èµ·ã“ã—å®Œäº†")
            return output

        except Exception as e:
            print(f"âŒ éŸ³å£°æ–‡å­—èµ·ã“ã—ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    @modal.method()
    def analyze_image(
        self,
        image_data: bytes,
        prompt: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        ç”»åƒåˆ†æï¼ˆClaude Vision or GPT-4Vï¼‰

        Args:
            image_data: ç”»åƒãƒ‡ãƒ¼ã‚¿ï¼ˆãƒã‚¤ãƒˆåˆ—ï¼‰
            prompt: ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ

        Returns:
            ç”»åƒåˆ†æçµæœ
        """
        import base64

        print(f"ğŸ–¼ï¸ ç”»åƒåˆ†æé–‹å§‹ï¼ˆãƒ—ãƒ©ã‚¤ãƒãƒª: {self.primary_llm_provider}ï¼‰")

        # Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        image_base64 = base64.b64encode(image_data).decode("utf-8")

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        if not prompt:
            prompt = """
ã“ã®ç”»åƒã‚’è©³ã—ãåˆ†æã—ã¦ãã ã•ã„ï¼š
1. ä½•ãŒæ˜ ã£ã¦ã„ã‚‹ã‹ï¼ˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€äººç‰©ã€é¢¨æ™¯ãªã©ï¼‰
2. æŠ€è¡“çš„ãªè¦ç´ ï¼ˆã‚³ãƒ¼ãƒ‰ã€å›³ã€UIã€ãƒ›ãƒ¯ã‚¤ãƒˆãƒœãƒ¼ãƒ‰ãªã©ï¼‰
3. é‡è¦ãªæƒ…å ±ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã€æ•°å­—ã€è¨˜å·ãªã©ï¼‰
4. å…¨ä½“çš„ãªã‚·ãƒ¼ãƒ³ã®èª¬æ˜

JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š
{
  "description": "å…¨ä½“çš„ãªèª¬æ˜",
  "objects": ["æ¤œå‡ºã•ã‚ŒãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãƒªã‚¹ãƒˆ"],
  "text": "ç”»åƒå†…ã®ãƒ†ã‚­ã‚¹ãƒˆ",
  "scene": "ã‚·ãƒ¼ãƒ³ã®ç¨®é¡",
  "technical_elements": ["æŠ€è¡“çš„ãªè¦ç´ "]
}
"""

        try:
            # ãƒ—ãƒ©ã‚¤ãƒãƒªLLMã§è©¦è¡Œ
            result = self._analyze_image_with_provider(
                image_base64, 
                prompt, 
                self.primary_llm_provider
            )
            return result

        except Exception as e:
            print(f"âš ï¸ ãƒ—ãƒ©ã‚¤ãƒãƒªLLMï¼ˆ{self.primary_llm_provider}ï¼‰ã‚¨ãƒ©ãƒ¼: {e}")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãŒæœ‰åŠ¹ãªå ´åˆ
            if self.enable_llm_fallback:
                fallback_provider = "anthropic" if self.primary_llm_provider == "openai" else "openai"
                print(f"ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {fallback_provider}ã§å†è©¦è¡Œ")
                
                try:
                    result = self._analyze_image_with_provider(
                        image_base64, 
                        prompt, 
                        fallback_provider
                    )
                    return result
                except Exception as fallback_error:
                    print(f"âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯LLMã‚‚ã‚¨ãƒ©ãƒ¼: {fallback_error}")
                    raise
            else:
                raise

    @modal.method()
    def detect_scene_changes(
        self,
        frames: List[bytes],
        threshold: float = 0.3,
    ) -> List[int]:
        """
        ã‚·ãƒ¼ãƒ³å¤‰åŒ–æ¤œå‡º

        Args:
            frames: ãƒ•ãƒ¬ãƒ¼ãƒ ç”»åƒã®ãƒªã‚¹ãƒˆ
            threshold: å¤‰åŒ–æ¤œå‡ºã®é–¾å€¤

        Returns:
            ã‚·ãƒ¼ãƒ³å¤‰åŒ–ãŒæ¤œå‡ºã•ã‚ŒãŸãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        """
        import cv2
        import numpy as np

        print(f"ğŸ¬ ã‚·ãƒ¼ãƒ³å¤‰åŒ–æ¤œå‡ºé–‹å§‹ï¼ˆ{len(frames)} ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰")

        try:
            changes = []
            prev_frame = None

            for i, frame_data in enumerate(frames):
                # ãƒã‚¤ãƒˆåˆ—ã‹ã‚‰ç”»åƒã«å¤‰æ›
                nparr = np.frombuffer(frame_data, np.uint8)
                frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

                if prev_frame is not None:
                    # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ æ¯”è¼ƒ
                    hist1 = cv2.calcHist([prev_frame], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])
                    hist2 = cv2.calcHist([frame], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])

                    cv2.normalize(hist1, hist1)
                    cv2.normalize(hist2, hist2)

                    # é¡ä¼¼åº¦è¨ˆç®—
                    similarity = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)

                    # é–¾å€¤ä»¥ä¸‹ãªã‚‰å¤‰åŒ–ã¨ã¿ãªã™
                    if similarity < (1.0 - threshold):
                        changes.append(i)
                        print(f"  ğŸ“ ãƒ•ãƒ¬ãƒ¼ãƒ  {i}: å¤‰åŒ–æ¤œå‡ºï¼ˆé¡ä¼¼åº¦: {similarity:.3f}ï¼‰")

                prev_frame = frame

            print(f"âœ… {len(changes)} ç®‡æ‰€ã®ã‚·ãƒ¼ãƒ³å¤‰åŒ–ã‚’æ¤œå‡º")
            return changes

        except Exception as e:
            print(f"âŒ ã‚·ãƒ¼ãƒ³å¤‰åŒ–æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            raise

    @modal.method()
    def generate_specification(
        self,
        context: Dict[str, Any],
    ) -> Dict[str, Any]:
        """
        ä»•æ§˜æ›¸ç”Ÿæˆ

        Args:
            context: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ï¼ˆæ–‡å­—èµ·ã“ã—ã€ç”»åƒãªã©ï¼‰

        Returns:
            ç”Ÿæˆã•ã‚ŒãŸä»•æ§˜æ›¸
        """
        print(f"ğŸ“„ ä»•æ§˜æ›¸ç”Ÿæˆé–‹å§‹ï¼ˆãƒ—ãƒ©ã‚¤ãƒãƒª: {self.primary_llm_provider}ï¼‰")

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰
        prompt = self._build_specification_prompt(context)

        try:
            # ãƒ—ãƒ©ã‚¤ãƒãƒªLLMã§è©¦è¡Œ
            result = self._generate_text_with_provider(
                prompt,
                self.primary_llm_provider,
                max_tokens=4096
            )
            
            specification_text = result["content"]
            
            output = {
                "title": self._extract_title(specification_text),
                "content": specification_text,
                "model": result["model"],
                "timestamp": datetime.now().isoformat(),
            }

            print("âœ… ä»•æ§˜æ›¸ç”Ÿæˆå®Œäº†")
            return output

        except Exception as e:
            print(f"âš ï¸ ãƒ—ãƒ©ã‚¤ãƒãƒªLLMï¼ˆ{self.primary_llm_provider}ï¼‰ã‚¨ãƒ©ãƒ¼: {e}")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãŒæœ‰åŠ¹ãªå ´åˆ
            if self.enable_llm_fallback:
                fallback_provider = "anthropic" if self.primary_llm_provider == "openai" else "openai"
                print(f"ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {fallback_provider}ã§å†è©¦è¡Œ")
                
                try:
                    result = self._generate_text_with_provider(
                        prompt,
                        fallback_provider,
                        max_tokens=4096
                    )
                    
                    specification_text = result["content"]
                    
                    output = {
                        "title": self._extract_title(specification_text),
                        "content": specification_text,
                        "model": result["model"],
                        "timestamp": datetime.now().isoformat(),
                    }

                    print("âœ… ä»•æ§˜æ›¸ç”Ÿæˆå®Œäº†ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰")
                    return output
                    
                except Exception as fallback_error:
                    print(f"âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯LLMã‚‚ã‚¨ãƒ©ãƒ¼: {fallback_error}")
                    raise
            else:
                raise

    @modal.method()
    def generate_document(
        self,
        context: Dict[str, Any],
        document_type: str = "auto",  # 'specification' | 'minutes' | 'memo' | 'auto'
        additional_prompt: str = "",
    ) -> Dict[str, Any]:
        """
        ãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚°ç”Ÿæˆï¼ˆä»•æ§˜æ›¸ãƒ»è­°äº‹éŒ²ãƒ»ãƒ¡ãƒ¢ï¼‰

        Args:
            context: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ï¼ˆæ–‡å­—èµ·ã“ã—ã€ç”»åƒãªã©ï¼‰
            document_type: ãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚¿ã‚¤ãƒ—ï¼ˆ'specification', 'minutes', 'memo', 'auto'ï¼‰
            additional_prompt: è¿½åŠ ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆä»»æ„ï¼‰

        Returns:
            ç”Ÿæˆã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
        """
        print(f"ğŸ“„ ãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚°ç”Ÿæˆé–‹å§‹ï¼ˆã‚¿ã‚¤ãƒ—: {document_type}ã€ãƒ—ãƒ©ã‚¤ãƒãƒª: {self.primary_llm_provider}ï¼‰")

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰
        prompt = self._build_document_prompt(context, document_type, additional_prompt)

        try:
            # ãƒ—ãƒ©ã‚¤ãƒãƒªLLMã§è©¦è¡Œ
            result = self._generate_text_with_provider(
                prompt,
                self.primary_llm_provider,
                max_tokens=4096
            )
            
            document_text = result["content"]
            
            # ãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®šï¼ˆautoã®å ´åˆï¼‰
            final_type = document_type
            if document_type == "auto":
                final_type = self._detect_document_type(document_text)
            
            output = {
                "title": self._extract_title(document_text),
                "content": document_text,
                "type": final_type,
                "model": result["model"],
                "timestamp": datetime.now().isoformat(),
            }

            print(f"âœ… ãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚°ç”Ÿæˆå®Œäº†ï¼ˆã‚¿ã‚¤ãƒ—: {final_type}ï¼‰")
            return output

        except Exception as e:
            print(f"âš ï¸ ãƒ—ãƒ©ã‚¤ãƒãƒªLLMï¼ˆ{self.primary_llm_provider}ï¼‰ã‚¨ãƒ©ãƒ¼: {e}")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãŒæœ‰åŠ¹ãªå ´åˆ
            if self.enable_llm_fallback:
                fallback_provider = "anthropic" if self.primary_llm_provider == "openai" else "openai"
                print(f"ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {fallback_provider}ã§å†è©¦è¡Œ")
                
                try:
                    result = self._generate_text_with_provider(
                        prompt,
                        fallback_provider,
                        max_tokens=4096
                    )
                    
                    document_text = result["content"]
                    
                    # ãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®šï¼ˆautoã®å ´åˆï¼‰
                    final_type = document_type
                    if document_type == "auto":
                        final_type = self._detect_document_type(document_text)
                    
                    output = {
                        "title": self._extract_title(document_text),
                        "content": document_text,
                        "type": final_type,
                        "model": result["model"],
                        "timestamp": datetime.now().isoformat(),
                    }

                    print(f"âœ… ãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚°ç”Ÿæˆå®Œäº†ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€ã‚¿ã‚¤ãƒ—: {final_type}ï¼‰")
                    return output
                    
                except Exception as fallback_error:
                    print(f"âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯LLMã‚‚ã‚¨ãƒ©ãƒ¼: {fallback_error}")
                    raise
            else:
                raise

    @modal.method()
    def generate_code(
        self,
        request: Dict[str, Any],
    ) -> Dict[str, Any]:
        """
        ã‚³ãƒ¼ãƒ‰ç”Ÿæˆ

        Args:
            request: ã‚³ãƒ¼ãƒ‰ç”Ÿæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆ
                - prompt: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚ï¼ˆå¿…é ˆï¼‰
                - context: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ï¼ˆæ–‡å­—èµ·ã“ã—ã€ä»•æ§˜æ›¸ãªã©ï¼‰
                - language: ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                - framework: ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

        Returns:
            ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰
        """
        print(f"ğŸ’» ã‚³ãƒ¼ãƒ‰ç”Ÿæˆé–‹å§‹ï¼ˆãƒ—ãƒ©ã‚¤ãƒãƒª: {self.primary_llm_provider}ï¼‰")

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰
        prompt = self._build_code_generation_prompt(request)

        try:
            # ãƒ—ãƒ©ã‚¤ãƒãƒªLLMã§è©¦è¡Œ
            result = self._generate_text_with_provider(
                prompt,
                self.primary_llm_provider,
                max_tokens=16384  # ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã¯é•·ã‚ã«ï¼ˆå®Œå…¨ãªREADMEç”Ÿæˆã®ãŸã‚å¢—é‡ï¼‰
            )
            
            code_text = result["content"]
            
            # ã‚³ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã®æŠ½å‡º
            files = self._extract_code_files(code_text)
            
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦ã®æŠ½å‡º
            project_summary = self._extract_project_summary(code_text)
            
            output = {
                "files": files,
                "dependencies": self._extract_dependencies(code_text),
                "instructions": self._extract_instructions(code_text),
                "summary": project_summary,
                "model": result["model"],
                "timestamp": datetime.now().isoformat(),
            }

            print(f"âœ… ã‚³ãƒ¼ãƒ‰ç”Ÿæˆå®Œäº†ï¼ˆ{len(files)}ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰")
            print(f"ğŸ“Š ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå: {project_summary['name']}")
            return output

        except Exception as e:
            print(f"âš ï¸ ãƒ—ãƒ©ã‚¤ãƒãƒªLLMï¼ˆ{self.primary_llm_provider}ï¼‰ã‚¨ãƒ©ãƒ¼: {e}")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãŒæœ‰åŠ¹ãªå ´åˆ
            if self.enable_llm_fallback:
                fallback_provider = "anthropic" if self.primary_llm_provider == "openai" else "openai"
                print(f"ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {fallback_provider}ã§å†è©¦è¡Œ")
                
                try:
                    result = self._generate_text_with_provider(
                        prompt,
                        fallback_provider,
                        max_tokens=16384
                    )
                    
                    code_text = result["content"]
                    files = self._extract_code_files(code_text)
                    project_summary = self._extract_project_summary(code_text)
                    
                    output = {
                        "files": files,
                        "dependencies": self._extract_dependencies(code_text),
                        "instructions": self._extract_instructions(code_text),
                        "summary": project_summary,
                        "model": result["model"],
                        "timestamp": datetime.now().isoformat(),
                    }

                    print(f"âœ… ã‚³ãƒ¼ãƒ‰ç”Ÿæˆå®Œäº†ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€{len(files)}ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰")
                    print(f"ğŸ“Š ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå: {project_summary['name']}")
                    return output
                    
                except Exception as fallback_error:
                    print(f"âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯LLMã‚‚ã‚¨ãƒ©ãƒ¼: {fallback_error}")
                    raise
            else:
                raise

    # ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰

    def _extract_speakers(self, result: Dict) -> List[str]:
        """è©±è€…ãƒªã‚¹ãƒˆã‚’æŠ½å‡º"""
        speakers = set()
        for segment in result.get("segments", []):
            if "speaker" in segment:
                speakers.add(segment["speaker"])
        return sorted(list(speakers))

    def _build_specification_prompt(self, context: Dict[str, Any]) -> str:
        """ä»•æ§˜æ›¸ç”Ÿæˆç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰"""
        transcriptions = context.get("transcriptions", [])
        photos = context.get("photos", [])

        prompt = """
ä»¥ä¸‹ã®ä¼šè­°ãƒ»ä½œæ¥­ã®è¨˜éŒ²ã‹ã‚‰ã€æŠ€è¡“ä»•æ§˜æ›¸ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

# æ–‡å­—èµ·ã“ã—
"""
        for trans in transcriptions[-20:]:  # æœ€æ–°20ä»¶
            speaker = trans.get("speaker", "ä¸æ˜")
            text = trans.get("text", "")
            prompt += f"\n[{speaker}] {text}"

        prompt += """

# è¦æ±‚äº‹é …
- æ˜ç¢ºã§æ§‹é€ åŒ–ã•ã‚ŒãŸä»•æ§˜æ›¸ã‚’ä½œæˆ
- è¦ä»¶ã€æŠ€è¡“è©³ç´°ã€å®Ÿè£…æ‰‹é †ã‚’å«ã‚ã‚‹
- Markdownå½¢å¼ã§å‡ºåŠ›

# å‡ºåŠ›å½¢å¼
## ã‚¿ã‚¤ãƒˆãƒ«
## æ¦‚è¦
## è¦ä»¶
### æ©Ÿèƒ½è¦ä»¶
### éæ©Ÿèƒ½è¦ä»¶
## æŠ€è¡“è©³ç´°
## å®Ÿè£…æ‰‹é †
## å‚™è€ƒ
"""
        return prompt

    def _build_document_prompt(
        self, 
        context: Dict[str, Any], 
        document_type: str,
        additional_prompt: str = ""
    ) -> str:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰"""
        transcriptions = context.get("transcriptions", [])
        photos = context.get("photos", [])

        # ãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        if document_type == "specification":
            type_instruction = """æŠ€è¡“ä»•æ§˜æ›¸ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
è¦ä»¶ã€æŠ€è¡“è©³ç´°ã€å®Ÿè£…æ‰‹é †ã‚’å«ã‚ã¦ãã ã•ã„ã€‚

## å‡ºåŠ›å½¢å¼
# ã‚¿ã‚¤ãƒˆãƒ«
## æ¦‚è¦
## æ©Ÿèƒ½è¦ä»¶
## éæ©Ÿèƒ½è¦ä»¶
## æŠ€è¡“è©³ç´°
## å®Ÿè£…æ‰‹é †
## å‚™è€ƒ"""
        elif document_type == "minutes":
            type_instruction = """è­°äº‹éŒ²ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
æ—¥æ™‚ã€å‚åŠ è€…ã€è­°é¡Œã€æ±ºå®šäº‹é …ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ã‚’å«ã‚ã¦ãã ã•ã„ã€‚

## å‡ºåŠ›å½¢å¼
# ã‚¿ã‚¤ãƒˆãƒ«
## ä¼šè­°æƒ…å ±
- æ—¥æ™‚: 
- å‚åŠ è€…: 
## è­°é¡Œ
## è¨è­°å†…å®¹
## æ±ºå®šäº‹é …
## ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ 
## æ¬¡å›äºˆå®š"""
        elif document_type == "memo":
            type_instruction = """ãƒ¡ãƒ¢ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’ç®‡æ¡æ›¸ãã§ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚

## å‡ºåŠ›å½¢å¼
# ã‚¿ã‚¤ãƒˆãƒ«
## æ¦‚è¦
## é‡è¦ãƒã‚¤ãƒ³ãƒˆ
## è£œè¶³"""
        else:  # auto
            type_instruction = """å†…å®¹ã«æœ€ã‚‚é©ã—ãŸå½¢å¼ï¼ˆä»•æ§˜æ›¸ãƒ»è­°äº‹éŒ²ãƒ»ãƒ¡ãƒ¢ï¼‰ã§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
æ§‹é€ åŒ–ã•ã‚ŒãŸMarkdownå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"""

        prompt = f"""
ä»¥ä¸‹ã®è¨˜éŒ²ã‹ã‚‰ã€{type_instruction}

# æ–‡å­—èµ·ã“ã—
"""
        for trans in transcriptions[-30:]:  # æœ€æ–°30ä»¶
            speaker = trans.get("speaker", "ä¸æ˜")
            text = trans.get("text", "")
            prompt += f"\n[{speaker}] {text}"

        if additional_prompt:
            prompt += f"""

# è¿½åŠ ã®æŒ‡ç¤º
{additional_prompt}
"""

        prompt += """

# è¦æ±‚äº‹é …
- æ˜ç¢ºã§æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆ
- **èª¬æ˜æ–‡ãªã—ã§ã€ç›´æ¥Markdownãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼ˆã€Œä»¥ä¸‹ã¯...ã€ãªã©ã®å‰ç½®ãã¯ä¸è¦ã§ã™ï¼‰**
- **Markdownã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆ```markdownï¼‰ã§å›²ã¾ãªã„ã§ãã ã•ã„ã€‚ç›´æ¥Markdownã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„**
- Markdownå½¢å¼ã§å‡ºåŠ›
- å¿…è¦ã«å¿œã˜ã¦è¡¨ã‚„ç®‡æ¡æ›¸ãã‚’ä½¿ç”¨
- **ãƒ•ãƒ­ãƒ¼å›³ã€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å›³ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å›³ã€ERå›³ãªã©ã®å›³ã‚’è¡¨ç¾ã™ã‚‹å ´åˆã¯ã€Mermaidè¨˜æ³•ã‚’ç©æ¥µçš„ã«ä½¿ç”¨ã—ã¦ãã ã•ã„**
- Mermaidã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã¯ ```mermaid ã§å›²ã‚“ã§ãã ã•ã„

## Mermaidè¨˜æ³•ã®ä¾‹:

### ãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ:
```mermaid
graph TD
    A[é–‹å§‹] --> B{æ¡ä»¶}
    B -->|Yes| C[å‡¦ç†1]
    B -->|No| D[å‡¦ç†2]
    C --> E[çµ‚äº†]
    D --> E
```

### ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å›³:
```mermaid
sequenceDiagram
    participant A as ãƒ¦ãƒ¼ã‚¶ãƒ¼
    participant B as ã‚µãƒ¼ãƒãƒ¼
    A->>B: ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
    B->>A: ãƒ¬ã‚¹ãƒãƒ³ã‚¹
```

### ERå›³:
```mermaid
erDiagram
    USER ||--o{ ORDER : places
    USER {
        string name
        string email
    }
    ORDER {
        int orderId
        date orderDate
    }
```

### ã‚¬ãƒ³ãƒˆãƒãƒ£ãƒ¼ãƒˆ:
```mermaid
gantt
    title ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
    section è¨­è¨ˆ
    è¦ä»¶å®šç¾© :a1, 2024-01-01, 7d
    åŸºæœ¬è¨­è¨ˆ :a2, after a1, 10d
    section é–‹ç™º
    å®Ÿè£…     :a3, after a2, 14d
```
"""
        return prompt

    def _detect_document_type(self, text: str) -> str:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ã‚’è‡ªå‹•åˆ¤å®š"""
        text_lower = text.lower()
        
        # è­°äº‹éŒ²ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        if any(keyword in text_lower for keyword in ["è­°äº‹éŒ²", "ä¼šè­°", "å‚åŠ è€…", "æ±ºå®šäº‹é …", "ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ", "minutes"]):
            return "minutes"
        
        # ä»•æ§˜æ›¸ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        if any(keyword in text_lower for keyword in ["ä»•æ§˜", "è¦ä»¶", "æ©Ÿèƒ½è¦ä»¶", "å®Ÿè£…æ‰‹é †", "specification", "requirements"]):
            return "specification"
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ãƒ¡ãƒ¢
        return "memo"

    def _extract_title(self, text: str) -> str:
        """ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º"""
        lines = text.split("\n")
        for line in lines:
            if line.startswith("# "):
                return line.replace("# ", "").strip()
        return "ä»•æ§˜æ›¸"

    def _build_code_generation_prompt(self, request: Dict[str, Any]) -> str:
        """ã‚³ãƒ¼ãƒ‰ç”Ÿæˆç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰"""
        user_prompt = request.get("prompt", "")
        context = request.get("context", {})
        language = request.get("language", "")
        framework = request.get("framework", "")
        
        transcriptions = context.get("transcriptions", [])
        specification = context.get("specification", "")
        
        prompt = f"""
ã‚ãªãŸã¯ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãªã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã™ã€‚
ä»¥ä¸‹ã®è¦æ±‚ã«åŸºã¥ã„ã¦ã€å®Ÿè£…å¯èƒ½ãªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚
{user_prompt}

"""
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’è¿½åŠ 
        if specification:
            prompt += f"""
# ä»•æ§˜æ›¸
{specification}

"""
        
        if transcriptions:
            prompt += """
# ä¼šè­°ã®æ–‡å­—èµ·ã“ã—
"""
            for trans in transcriptions[-10:]:  # æœ€æ–°10ä»¶
                speaker = trans.get("speaker", "ä¸æ˜")
                text = trans.get("text", "")
                prompt += f"\n[{speaker}] {text}"
            prompt += "\n"
        
        # è¨€èªãƒ»ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯æŒ‡å®š
        if language:
            prompt += f"""
# ä½¿ç”¨ã™ã‚‹è¨€èª
{language}
"""
        
        if framework:
            prompt += f"""
# ä½¿ç”¨ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
{framework}
"""
        
        prompt += """

---

# å‡ºåŠ›å½¢å¼

ä»¥ä¸‹ã®å½¢å¼ã§**å¿…ãšå³å¯†ã«**å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯**å¿…é ˆ**ã§ã™ã€‚
**çµ¶å¯¾ã«çœç•¥ã›ãšã€ã™ã¹ã¦ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Œå…¨ã«å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚**

## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦
ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå: ã€è¦æ±‚å†…å®¹ã«åŸºã¥ã„ãŸå…·ä½“çš„ãªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã€‘
èª¬æ˜: ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ç›®çš„ã¨æ©Ÿèƒ½ã‚’1-2è¡Œã§ç°¡æ½”ã«èª¬æ˜ã€‘
ä¸»ãªæ©Ÿèƒ½:
- ã€æ©Ÿèƒ½1: å…·ä½“çš„ãªèª¬æ˜ã€‘
- ã€æ©Ÿèƒ½2: å…·ä½“çš„ãªèª¬æ˜ã€‘
- ã€æ©Ÿèƒ½3: å…·ä½“çš„ãªèª¬æ˜ã€‘

## æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯
- è¨€èª: ã€ä½¿ç”¨è¨€èªã€‘
- ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯: ã€ä½¿ç”¨ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼ˆã‚ã‚‹å ´åˆï¼‰ã€‘
- ä¸»è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒª: ã€ä¸»è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§ã€‘

## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ 
```mermaid
graph TD
    A[ã€ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆåã€‘] --> B[ã€ä¸»è¦ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åã€‘]
    B --> C[ã€ã‚µãƒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«1åã€‘]
    B --> D[ã€ã‚µãƒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«2åã€‘]
    C --> E[ã€ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£åã€‘]
    D --> E
    
    style A fill:#667eea
    style B fill:#764ba2
    style C fill:#f093fb
    style D fill:#f093fb
    style E fill:#4facfe
```

## ä¾å­˜é–¢ä¿‚
```
ã€ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸1==ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã€‘
ã€ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸2==ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã€‘
ã€ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸3==ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã€‘
```

---

## ã‚³ãƒ¼ãƒ‰

### ãƒ•ã‚¡ã‚¤ãƒ«: README.md
```markdown
# ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåï¼ˆè¦æ±‚å†…å®¹ã«åŸºã¥ã„ãŸå…·ä½“çš„ãªåå‰ï¼‰ã€‘

<div align="center">

![Version](https://img.shields.io/badge/version-1.0.0-blue.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)
![Language](https://img.shields.io/badge/ã€è¨€èªã€‘-ã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã€‘-orange.svg)

ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ç°¡æ½”ãªèª¬æ˜ï¼ˆ1è¡Œï¼‰ã€‘

</div>

---

## ğŸ“‹ æ¦‚è¦

ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ç›®çš„ã€èƒŒæ™¯ã€è§£æ±ºã™ã‚‹èª²é¡Œã‚’3-5è¡Œã§èª¬æ˜ã€‘

### ãªãœã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒå¿…è¦ã‹ï¼Ÿ

- ğŸ“Œ ã€è§£æ±ºã™ã‚‹å•é¡Œ1ã€‘
- ğŸ“Œ ã€è§£æ±ºã™ã‚‹å•é¡Œ2ã€‘
- ğŸ“Œ ã€è§£æ±ºã™ã‚‹å•é¡Œ3ã€‘

## âœ¨ ä¸»ãªæ©Ÿèƒ½

| æ©Ÿèƒ½ | èª¬æ˜ |
|------|------|
| ğŸ¯ **ã€æ©Ÿèƒ½1ã€‘** | ã€è©³ç´°èª¬æ˜ã€‘ |
| ğŸš€ **ã€æ©Ÿèƒ½2ã€‘** | ã€è©³ç´°èª¬æ˜ã€‘ |
| ğŸ’¡ **ã€æ©Ÿèƒ½3ã€‘** | ã€è©³ç´°èª¬æ˜ã€‘ |

## ğŸ—ï¸ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

ã€ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯å¿…é ˆã§ã™ã€‚å¿…ãš2ã¤ã®Mermaidå›³ã‚’å«ã‚ã¦ãã ã•ã„ã€‘

### ã‚·ã‚¹ãƒ†ãƒ æ§‹æˆå›³

\```mermaid
graph TD
    A[ã€ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆåã€‘] -->|å…¥åŠ›| B[ã€ãƒ¡ã‚¤ãƒ³ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åã€‘]
    B -->|å‡¦ç†| C[ã€ã‚µãƒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«1åã€‘]
    B -->|å‡¦ç†| D[ã€ã‚µãƒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«2åã€‘]
    C -->|åˆ©ç”¨| E[ã€ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£åã€‘]
    D -->|åˆ©ç”¨| E
    
    style A fill:#667eea,stroke:#333,stroke-width:2px,color:#fff
    style B fill:#764ba2,stroke:#333,stroke-width:2px,color:#fff
    style C fill:#f093fb,stroke:#333,stroke-width:2px
    style D fill:#f5576c,stroke:#333,stroke-width:2px,color:#fff
    style E fill:#4facfe,stroke:#333,stroke-width:2px
\```

### ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼

ã€å¿…é ˆ: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å›³ã‚’å¿…ãšå«ã‚ã¦ãã ã•ã„ã€‘

\```mermaid
sequenceDiagram
    participant User as ãƒ¦ãƒ¼ã‚¶ãƒ¼
    participant App as ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
    participant Logic as å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯
    
    User->>App: ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
    App->>Logic: å‡¦ç†ä¾é ¼
    Logic-->>App: çµæœ
    App-->>User: ãƒ¬ã‚¹ãƒãƒ³ã‚¹
\```

## ğŸ› ï¸ æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯

- ğŸ”¤ **è¨€èª**: ã€è¨€èªã€‘ ã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã€‘
- âš¡ **ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**: ã€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€‘
- ğŸ“¦ **ä¸»è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒª**: ã€ãƒ©ã‚¤ãƒ–ãƒ©ãƒª1ã€‘, ã€ãƒ©ã‚¤ãƒ–ãƒ©ãƒª2ã€‘, ã€ãƒ©ã‚¤ãƒ–ãƒ©ãƒª3ã€‘

## ğŸ“¦ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

ã€ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯å¿…é ˆã§ã™ã€‚çœç•¥ã—ãªã„ã§ãã ã•ã„ã€‘

### å‰ææ¡ä»¶

- âœ… ã€è¨€èªåã€‘ ã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã€‘ä»¥ä¸Š
- âœ… ã€ãã®ä»–ã®ä¾å­˜é–¢ä¿‚ã€‘

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ‰‹é †

\```bash
# 1. ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³
git clone https://github.com/yourusername/project-name.git
cd project-name

# 2. ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
ã€ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚³ãƒãƒ³ãƒ‰ï¼ˆä¾‹: pip install -r requirements.txtï¼‰ã€‘

# 3. ç’°å¢ƒå¤‰æ•°ã®è¨­å®šï¼ˆå¿…è¦ãªå ´åˆï¼‰
cp .env.example .env

# 4. åˆæœŸåŒ–
ã€åˆæœŸåŒ–ã‚³ãƒãƒ³ãƒ‰ï¼ˆã‚ã‚‹å ´åˆï¼‰ã€‘
\```

## ğŸš€ ä½¿ã„æ–¹

ã€ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯å¿…é ˆã§ã™ã€‚çœç•¥ã—ãªã„ã§ãã ã•ã„ã€‘

### ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

\```bash
# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’èµ·å‹•
ã€å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ï¼ˆä¾‹: python app.pyï¼‰ã€‘

# ãƒ–ãƒ©ã‚¦ã‚¶ã§é–‹ãï¼ˆWebã‚¢ãƒ—ãƒªã®å ´åˆï¼‰
# http://localhost:ã€ãƒãƒ¼ãƒˆç•ªå·ã€‘
\```

### åŸºæœ¬çš„ãªä½¿ç”¨ä¾‹

\```bash
# ä¾‹1: ã€ä½¿ç”¨ä¾‹1ã®èª¬æ˜ã€‘
ã€ã‚³ãƒãƒ³ãƒ‰ä¾‹1ã€‘

# ä¾‹2: ã€ä½¿ç”¨ä¾‹2ã®èª¬æ˜ã€‘
ã€ã‚³ãƒãƒ³ãƒ‰ä¾‹2ã€‘
\```

## ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

ã€ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯å¿…é ˆã§ã™ã€‚çœç•¥ã—ãªã„ã§ãã ã•ã„ã€‘

\```
ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã€‘/
â”‚
â”œâ”€â”€ README.md              # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ ã€ä¾å­˜é–¢ä¿‚ãƒ•ã‚¡ã‚¤ãƒ«ã€‘    # requirements.txt ãªã©
â”œâ”€â”€ ã€ãƒ¡ã‚¤ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã€‘      # app.py / main.py ãªã©
â””â”€â”€ ã€ãã®ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ«ã€‘
\```

## ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

ã€ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯å¿…é ˆã§ã™ã€‚æœ€ä½2ã¤ã®å•é¡Œã¨è§£æ±ºæ–¹æ³•ã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‘

### å•é¡Œ1: ã€å•é¡Œã®å†…å®¹ã€‘

**ç—‡çŠ¶**: ã€ç—‡çŠ¶ã®èª¬æ˜ã€‘

**è§£æ±ºæ–¹æ³•**:
\```bash
ã€è§£æ±ºã‚³ãƒãƒ³ãƒ‰ã€‘
\```

### å•é¡Œ2: ã€å•é¡Œã®å†…å®¹ã€‘

**ç—‡çŠ¶**: ã€ç—‡çŠ¶ã®èª¬æ˜ã€‘

**è§£æ±ºæ–¹æ³•**: ã€è§£æ±ºæ‰‹é †ã€‘

## ğŸ“„ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

MIT License - è©³ç´°ã¯ [LICENSE](LICENSE) ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

---

<div align="center">
Made with â¤ï¸
</div>
\```

### ãƒ•ã‚¡ã‚¤ãƒ«: ã€ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹1ï¼ˆä¾‹: app.pyï¼‰ã€‘
```ã€è¨€èªåã€‘
ã€å®Ÿè£…ã‚³ãƒ¼ãƒ‰ - å¿…è¦æœ€ä½é™ã ãŒå‹•ä½œã™ã‚‹å®Œå…¨ãªã‚³ãƒ¼ãƒ‰ã€‘
ã€è©³ç´°ãªã‚³ãƒ¡ãƒ³ãƒˆã‚’å«ã‚ã‚‹ã€‘
ã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’å®Ÿè£…ã€‘
```

### ãƒ•ã‚¡ã‚¤ãƒ«: ã€ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹2ã€‘
```ã€è¨€èªåã€‘
ã€å®Ÿè£…ã‚³ãƒ¼ãƒ‰ã€‘
```

### ãƒ•ã‚¡ã‚¤ãƒ«: requirements.txt
```
ã€ä¾å­˜é–¢ä¿‚ãƒªã‚¹ãƒˆã€‘
```

---

## ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †
1. ã€å…·ä½“çš„ãªæ‰‹é †1ã€‘
2. ã€å…·ä½“çš„ãªæ‰‹é †2ã€‘
3. ã€å…·ä½“çš„ãªæ‰‹é †3ã€‘

## å®Ÿè¡Œæ–¹æ³•
\```bash
# ã€å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ã®èª¬æ˜ã€‘
ã€å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ã€‘
\```

---

**ğŸš¨ å¿…é ˆãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ - ä»¥ä¸‹ã‚’ã™ã¹ã¦æº€ãŸã—ã¦ãã ã•ã„:**

README.mdã«å¿…ãšå«ã‚ã‚‹ã“ã¨:
- âœ… ğŸ“‹ æ¦‚è¦ã‚»ã‚¯ã‚·ãƒ§ãƒ³
- âœ… âœ¨ ä¸»ãªæ©Ÿèƒ½ã‚»ã‚¯ã‚·ãƒ§ãƒ³
- âœ… ğŸ—ï¸ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆ**Mermaidå›³2ã¤å¿…é ˆ**ï¼‰
  - ã‚·ã‚¹ãƒ†ãƒ æ§‹æˆå›³ï¼ˆgraph TDå½¢å¼ï¼‰
  - ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼å›³ï¼ˆsequenceDiagramå½¢å¼ï¼‰
- âœ… ğŸ› ï¸ æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯ã‚»ã‚¯ã‚·ãƒ§ãƒ³
- âœ… ğŸ“¦ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆå…·ä½“çš„ãªã‚³ãƒãƒ³ãƒ‰ä»˜ãï¼‰
- âœ… ğŸš€ ä½¿ã„æ–¹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆå…·ä½“çš„ãªå®Ÿè¡Œä¾‹ä»˜ãï¼‰
- âœ… ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆã‚»ã‚¯ã‚·ãƒ§ãƒ³
- âœ… ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆæœ€ä½2ã¤ï¼‰
- âœ… ğŸ“„ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã‚»ã‚¯ã‚·ãƒ§ãƒ³

ãã®ä»–ã®è¦ä»¶:
1. âœ… ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã¯è¦æ±‚å†…å®¹ã‚’åæ˜ ã—ãŸå…·ä½“çš„ãªåå‰
2. âœ… README.mdã‚’**å¿…ãšæœ€åˆã«**ç”Ÿæˆ
3. âœ… Mermaidå›³ã¯å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ ã‚’æ­£ç¢ºã«åæ˜ 
4. âœ… ã‚³ãƒ¼ãƒ‰ã¯å®Œå…¨ã«å‹•ä½œã™ã‚‹çŠ¶æ…‹
5. âœ… ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã€ãƒ­ã‚°ã€è©³ç´°ãªã‚³ãƒ¡ãƒ³ãƒˆã‚’å«ã‚ã‚‹

**é‡è¦: ã™ã¹ã¦ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’çœç•¥ã›ãšå®Œå…¨ã«å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚ç‰¹ã«Mermaidå›³ã€ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã€ä½¿ã„æ–¹ã€ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆã€ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã€ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯å¿…é ˆã§ã™ã€‚**

---

âš ï¸ **å‡ºåŠ›å‰ã®æœ€çµ‚ç¢ºèª**

README.mdã‚’ç”Ÿæˆã™ã‚‹å‰ã«ã€ä»¥ä¸‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼š
1. ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«2ã¤ã®Mermaidå›³ï¼ˆã‚·ã‚¹ãƒ†ãƒ æ§‹æˆå›³ + ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼å›³ï¼‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ï¼Ÿ
2. ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«å…·ä½“çš„ãªbashã‚³ãƒãƒ³ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ï¼Ÿ
3. ä½¿ã„æ–¹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«å…·ä½“çš„ãªå®Ÿè¡Œä¾‹ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ï¼Ÿ
4. ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ„ãƒªãƒ¼ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ï¼Ÿ
5. ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«æœ€ä½2ã¤ã®å•é¡Œã¨è§£æ±ºæ–¹æ³•ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ï¼Ÿ
6. ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ï¼Ÿ

ã™ã¹ã¦ã€Œã¯ã„ã€ã®å ´åˆã®ã¿ã€å‡ºåŠ›ã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚
é€”ä¸­ã§æ­¢ã‚ãšã€æœ€å¾Œã¾ã§å®Œå…¨ã«å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
"""
        
        return prompt

    def _extract_code_files(self, text: str) -> List[Dict[str, str]]:
        """
        ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚³ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŠ½å‡º
        
        å½¢å¼: ### ãƒ•ã‚¡ã‚¤ãƒ«: path/to/file.ext
              ```language
              code here
              ```
        """
        import re
        
        files = []
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³: ### ãƒ•ã‚¡ã‚¤ãƒ«: path/to/file.ext ã®å¾Œã« ``` ã§å›²ã¾ã‚ŒãŸã‚³ãƒ¼ãƒ‰
        pattern = r'###\s*ãƒ•ã‚¡ã‚¤ãƒ«:\s*([^\n]+)\s*```(\w+)?\s*\n(.*?)```'
        matches = re.finditer(pattern, text, re.DOTALL)
        
        for match in matches:
            filepath = match.group(1).strip()
            language = match.group(2) or self._detect_language_from_path(filepath)
            content = match.group(3).strip()
            
            files.append({
                "path": filepath,
                "content": content,
                "language": language,
            })
            
            print(f"  ğŸ“„ æŠ½å‡º: {filepath} ({language})")
        
        # ä»£æ›¿ãƒ‘ã‚¿ãƒ¼ãƒ³: ```language:filename
        alt_pattern = r'```(\w+):([^\n]+)\s*\n(.*?)```'
        alt_matches = re.finditer(alt_pattern, text, re.DOTALL)
        
        for match in alt_matches:
            language = match.group(1).strip()
            filepath = match.group(2).strip()
            content = match.group(3).strip()
            
            # æ—¢ã«åŒã˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒæŠ½å‡ºã•ã‚Œã¦ã„ãªã„ã‹ãƒã‚§ãƒƒã‚¯
            if not any(f["path"] == filepath for f in files):
                files.append({
                    "path": filepath,
                    "content": content,
                    "language": language,
                })
                print(f"  ğŸ“„ æŠ½å‡ºï¼ˆä»£æ›¿å½¢å¼ï¼‰: {filepath} ({language})")
        
        return files

    def _extract_dependencies(self, text: str) -> List[str]:
        """ä¾å­˜é–¢ä¿‚ã‚’æŠ½å‡º"""
        import re
        
        dependencies = []
        
        # ## ä¾å­˜é–¢ä¿‚ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¢ã™
        dep_pattern = r'##\s*ä¾å­˜é–¢ä¿‚\s*```[^\n]*\n(.*?)```'
        match = re.search(dep_pattern, text, re.DOTALL)
        
        if match:
            dep_text = match.group(1).strip()
            # å„è¡Œã‚’ä¾å­˜é–¢ä¿‚ã¨ã—ã¦è¿½åŠ 
            for line in dep_text.split('\n'):
                line = line.strip()
                if line and not line.startswith('#'):
                    dependencies.append(line)
        
        return dependencies

    def _extract_instructions(self, text: str) -> str:
        """ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †ã¨å®Ÿè¡Œæ–¹æ³•ã‚’æŠ½å‡º"""
        import re
        
        instructions = []
        
        # ## ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é † ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¢ã™
        setup_pattern = r'##\s*ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †\s*\n(.*?)(?=##|$)'
        setup_match = re.search(setup_pattern, text, re.DOTALL)
        
        if setup_match:
            instructions.append("# ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †")
            instructions.append(setup_match.group(1).strip())
        
        # ## å®Ÿè¡Œæ–¹æ³• ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¢ã™
        run_pattern = r'##\s*å®Ÿè¡Œæ–¹æ³•\s*\n(.*?)(?=##|$)'
        run_match = re.search(run_pattern, text, re.DOTALL)
        
        if run_match:
            instructions.append("\n# å®Ÿè¡Œæ–¹æ³•")
            instructions.append(run_match.group(1).strip())
        
        return "\n".join(instructions) if instructions else "ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †ã¯ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã«å«ã¾ã‚Œã¦ã„ã¾ã™ã€‚"

    def _detect_language_from_path(self, filepath: str) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰è¨€èªã‚’æ¨æ¸¬"""
        ext_map = {
            '.py': 'python',
            '.js': 'javascript',
            '.ts': 'typescript',
            '.jsx': 'javascript',
            '.tsx': 'typescript',
            '.java': 'java',
            '.cpp': 'cpp',
            '.c': 'c',
            '.go': 'go',
            '.rs': 'rust',
            '.rb': 'ruby',
            '.php': 'php',
            '.swift': 'swift',
            '.kt': 'kotlin',
            '.scala': 'scala',
            '.sh': 'bash',
            '.html': 'html',
            '.css': 'css',
            '.json': 'json',
            '.yaml': 'yaml',
            '.yml': 'yaml',
            '.xml': 'xml',
            '.md': 'markdown',
        }
        
        import os
        _, ext = os.path.splitext(filepath)
        return ext_map.get(ext.lower(), 'text')
    
    def _extract_project_summary(self, text: str) -> Dict[str, Any]:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦ã‚’æŠ½å‡º"""
        import re
        
        summary = {
            "name": "ç”Ÿæˆã•ã‚ŒãŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ",
            "description": "",
            "features": [],
            "tech_stack": {
                "language": "",
                "framework": "",
                "libraries": []
            },
            "mermaid_diagram": ""
        }
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå
        name_pattern = r'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå:\s*(?:ã€)?([^ã€‘\n]+)(?:ã€‘)?'
        name_match = re.search(name_pattern, text)
        if name_match:
            summary["name"] = name_match.group(1).strip()
        
        # èª¬æ˜
        desc_pattern = r'èª¬æ˜:\s*(?:ã€)?([^ã€‘\n]+)(?:ã€‘)?'
        desc_match = re.search(desc_pattern, text)
        if desc_match:
            summary["description"] = desc_match.group(1).strip()
        
        # ä¸»ãªæ©Ÿèƒ½
        features_pattern = r'ä¸»ãªæ©Ÿèƒ½:?\s*\n((?:-\s+[^\n]+\n?)+)'
        features_match = re.search(features_pattern, text)
        if features_match:
            for line in features_match.group(1).split('\n'):
                if line.strip().startswith('-'):
                    feature = line.strip()[1:].strip()
                    if feature:
                        summary["features"].append(feature)
        
        # æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯
        lang_pattern = r'-\s*è¨€èª:\s*(?:ã€)?([^ã€‘\n]+)(?:ã€‘)?'
        lang_match = re.search(lang_pattern, text)
        if lang_match:
            summary["tech_stack"]["language"] = lang_match.group(1).strip()
        
        fw_pattern = r'-\s*ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯:\s*(?:ã€)?([^ã€‘\n]+)(?:ã€‘)?'
        fw_match = re.search(fw_pattern, text)
        if fw_match:
            summary["tech_stack"]["framework"] = fw_match.group(1).strip()
        
        lib_pattern = r'-\s*ä¸»è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒª:\s*(?:ã€)?([^ã€‘\n]+)(?:ã€‘)?'
        lib_match = re.search(lib_pattern, text)
        if lib_match:
            libs = lib_match.group(1).strip()
            summary["tech_stack"]["libraries"] = [l.strip() for l in libs.split(',') if l.strip()]
        
        # Mermaidå›³
        mermaid_pattern = r'```mermaid\s*\n(.*?)```'
        mermaid_match = re.search(mermaid_pattern, text, re.DOTALL)
        if mermaid_match:
            summary["mermaid_diagram"] = mermaid_match.group(1).strip()
        
        return summary

    def _analyze_image_with_provider(
        self,
        image_base64: str,
        prompt: str,
        provider: str,
    ) -> Dict[str, Any]:
        """
        æŒ‡å®šã•ã‚ŒãŸãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§ç”»åƒåˆ†æã‚’å®Ÿè¡Œ

        Args:
            image_base64: Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒ
            prompt: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            provider: 'openai' ã¾ãŸã¯ 'anthropic'

        Returns:
            ç”»åƒåˆ†æçµæœ
        """
        if provider == "openai":
            from openai import OpenAI
            
            print(f"  ğŸ¤– OpenAI ({self.openai_model})ã§ç”»åƒåˆ†æä¸­...")
            client = OpenAI(api_key=self.openai_api_key)
            
            response = client.chat.completions.create(
                model=self.openai_model,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": prompt
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{image_base64}"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=1024,
            )
            
            return {
                "analysis": response.choices[0].message.content,
                "model": self.openai_model,
                "timestamp": datetime.now().isoformat(),
            }
            
        elif provider == "anthropic":
            from anthropic import Anthropic
            
            print(f"  ğŸ¤– Anthropic ({self.anthropic_model})ã§ç”»åƒåˆ†æä¸­...")
            client = Anthropic(api_key=self.anthropic_api_key)
            
            response = client.messages.create(
                model=self.anthropic_model,
                max_tokens=1024,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "image",
                                "source": {
                                    "type": "base64",
                                    "media_type": "image/jpeg",
                                    "data": image_base64,
                                },
                            },
                            {"type": "text", "text": prompt},
                        ],
                    }
                ],
            )
            
            return {
                "analysis": response.content[0].text,
                "model": self.anthropic_model,
                "timestamp": datetime.now().isoformat(),
            }
        else:
            raise ValueError(f"æœªã‚µãƒãƒ¼ãƒˆã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {provider}")

    def _generate_text_with_provider(
        self,
        prompt: str,
        provider: str,
        max_tokens: int = 4096,
    ) -> Dict[str, Any]:
        """
        æŒ‡å®šã•ã‚ŒãŸãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’å®Ÿè¡Œ

        Args:
            prompt: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            provider: 'openai' ã¾ãŸã¯ 'anthropic'
            max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°

        Returns:
            ç”Ÿæˆçµæœ
        """
        if provider == "openai":
            from openai import OpenAI
            
            print(f"  ğŸ¤– OpenAI ({self.openai_model})ã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆä¸­...")
            client = OpenAI(api_key=self.openai_api_key)
            
            response = client.chat.completions.create(
                model=self.openai_model,
                messages=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                max_tokens=max_tokens,
            )
            
            return {
                "content": response.choices[0].message.content,
                "model": self.openai_model,
            }
            
        elif provider == "anthropic":
            from anthropic import Anthropic
            
            print(f"  ğŸ¤– Anthropic ({self.anthropic_model})ã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆä¸­...")
            client = Anthropic(api_key=self.anthropic_api_key)
            
            response = client.messages.create(
                model=self.anthropic_model,
                max_tokens=max_tokens,
                messages=[
                    {
                        "role": "user",
                        "content": prompt,
                    }
                ],
            )
            
            return {
                "content": response.content[0].text,
                "model": self.anthropic_model,
            }
        else:
            raise ValueError(f"æœªã‚µãƒãƒ¼ãƒˆã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {provider}")


# FastAPI Webã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.function(image=image, secrets=secrets)
@modal.asgi_app()
def fastapi_app():
    """
    FastAPI Webã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    """
    from fastapi import FastAPI, UploadFile, File, Form, Request
    from fastapi.responses import JSONResponse

    web_app = FastAPI(title="Realworld Agent GPU API")

    @web_app.get("/")
    async def root():
        return {
            "name": "Realworld Agent GPU Server",
            "version": "0.1.0",
            "status": "running",
        }

    @web_app.get("/health")
    async def health():
        return {"status": "healthy"}

    @web_app.post("/api/transcribe")
    async def transcribe(
        audio: UploadFile = File(...),
        language: str = Form("ja"),
        enable_diarization: bool = Form(True),
    ):
        """éŸ³å£°æ–‡å­—èµ·ã“ã—API"""
        try:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            temp_path = f"/tmp/{audio.filename}"
            with open(temp_path, "wb") as f:
                f.write(await audio.read())

            # GPUã‚¯ãƒ©ã‚¹ã‚’å‘¼ã³å‡ºã—
            gpu = RealworldAgentGPU()
            result = gpu.transcribe_audio.remote(
                temp_path,
                language=language,
                enable_diarization=enable_diarization,
            )

            return JSONResponse(content=result)

        except Exception as e:
            return JSONResponse(
                status_code=500,
                content={"error": str(e)},
            )

    @web_app.post("/api/analyze-image")
    async def analyze_image(image: UploadFile = File(...)):
        """ç”»åƒåˆ†æAPI"""
        try:
            image_data = await image.read()

            gpu = RealworldAgentGPU()
            result = gpu.analyze_image.remote(image_data)

            return JSONResponse(content=result)

        except Exception as e:
            return JSONResponse(
                status_code=500,
                content={"error": str(e)},
            )

    @web_app.post("/generate-spec")
    async def generate_specification(request: Request):
        """ä»•æ§˜æ›¸ç”ŸæˆAPI"""
        try:
            body = await request.json()
            context = body.get("context", {})

            gpu = RealworldAgentGPU()
            result = gpu.generate_specification.remote(context)

            return JSONResponse(content=result)

        except Exception as e:
            return JSONResponse(
                status_code=500,
                content={"error": str(e)},
            )

    @web_app.post("/api/generate-code")
    async def generate_code(request: Request):
        """ã‚³ãƒ¼ãƒ‰ç”ŸæˆAPI"""
        try:
            body = await request.json()

            gpu = RealworldAgentGPU()
            result = gpu.generate_code.remote(body)

            return JSONResponse(content=result)

        except Exception as e:
            return JSONResponse(
                status_code=500,
                content={"error": str(e)},
            )

    @web_app.post("/api/generate-document")
    async def generate_document(request: Request):
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”ŸæˆAPIï¼ˆä»•æ§˜æ›¸ãƒ»è­°äº‹éŒ²ãƒ»ãƒ¡ãƒ¢ï¼‰"""
        try:
            body = await request.json()
            context = body.get("context", {})
            document_type = body.get("document_type", "auto")
            additional_prompt = body.get("additional_prompt", "")

            gpu = RealworldAgentGPU()
            result = gpu.generate_document.remote(
                context, document_type, additional_prompt
            )

            return JSONResponse(content=result)

        except Exception as e:
            return JSONResponse(
                status_code=500,
                content={"error": str(e)},
            )

    return web_app

