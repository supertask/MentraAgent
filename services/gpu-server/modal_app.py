"""
Modal GPU Server Application
WhisperXã€pyannoteã€Vision LLMã‚’ä½¿ç”¨ã—ãŸé«˜åº¦ãªAIå‡¦ç†
"""

import modal
import os
from typing import Dict, List, Any, Optional
from datetime import datetime

# Modalã‚¢ãƒ—ãƒªã®ä½œæˆ
app = modal.App("realworld-agent")

# GPUç’°å¢ƒã¨ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”¨ã®Volume
models_volume = modal.Volume.from_name("realworld-agent-models", create_if_missing=True)

# Docker ã‚¤ãƒ¡ãƒ¼ã‚¸ã®å®šç¾©
image = (
    modal.Image.debian_slim(python_version="3.11")
    .apt_install(
        "ffmpeg",
        "git",
        "pkg-config",
        "libavformat-dev",
        "libavcodec-dev",
        "libavdevice-dev",
        "libavutil-dev",
        "libswscale-dev",
        "libswresample-dev",
        "libavfilter-dev",
    )
    .pip_install(
        # ä»•æ§˜æ›¸ç”Ÿæˆã«å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ï¼ˆæœ€å°æ§‹æˆï¼‰
        "anthropic>=0.40.0",
        "openai>=1.54.0",
        "Pillow>=10.2.0",
        "python-dotenv>=1.0.1",
        "fastapi[all]>=0.109.2",
        "pydantic>=2.6.1",
        # éŸ³å£°æ–‡å­—èµ·ã“ã—ç”¨ï¼ˆç¾åœ¨ã¯ç„¡åŠ¹åŒ–ï¼‰
        # "torch==2.1.0",
        # "torchaudio==2.1.0",
        # "whisperx==3.1.1",
        # "pyannote.audio==3.1.1",
        # "openai-whisper==20231117",
        # "faster-whisper==0.9.0",
        # "numpy==1.24.3",
        # "scipy==1.11.4",
        # "scikit-learn==1.3.2",
        # "opencv-python-headless==4.8.1.78",
    )
)

# Secretsï¼ˆç’°å¢ƒå¤‰æ•°ï¼‰
secrets = [
    modal.Secret.from_name("realworld-agent-secrets"),
]


@app.cls(
    image=image,
    # gpu="A10G",  # GPUã¯éŸ³å£°æ–‡å­—èµ·ã“ã—æ™‚ã®ã¿å¿…è¦ï¼ˆç¾åœ¨ã¯ç„¡åŠ¹åŒ–ï¼‰
    secrets=secrets,
    volumes={"/models": models_volume},
    timeout=600,  # 10åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
    # keep_warm=1,  # ã‚³ã‚¹ãƒˆå‰Šæ¸›ã®ãŸã‚ç„¡åŠ¹åŒ–ï¼ˆã‚³ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ¼ãƒˆã‚ã‚Šï¼š20-60ç§’ï¼‰
)
class RealworldAgentGPU:
    """
    GPUã‚’ä½¿ç”¨ã—ãŸAIå‡¦ç†ã‚¯ãƒ©ã‚¹
    """

    def __init__(self):
        """åˆæœŸåŒ–"""
        self.whisperx_model = None
        self.diarization_pipeline = None
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.anthropic_api_key = os.getenv("ANTHROPIC_API_KEY")
        self.primary_llm_provider = os.getenv("PRIMARY_LLM_PROVIDER", "openai")
        self.enable_llm_fallback = os.getenv("ENABLE_LLM_FALLBACK", "true").lower() == "true"
        self.openai_model = os.getenv("OPENAI_MODEL", "gpt-4o")
        self.anthropic_model = os.getenv("ANTHROPIC_MODEL", "claude-3-5-sonnet-20241022")

    @modal.enter()
    def load_models(self):
        """ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆèµ·å‹•æ™‚ã«1å›ã ã‘å®Ÿè¡Œï¼‰"""
        print("ğŸš€ åˆæœŸåŒ–ä¸­...")
        print("âœ… åˆæœŸåŒ–å®Œäº†")
        print("â„¹ï¸  WhisperX/pyannoteã¯ç¾åœ¨ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™ï¼ˆä»•æ§˜æ›¸ç”Ÿæˆã®ã¿åˆ©ç”¨å¯èƒ½ï¼‰")

    @modal.method()
    def transcribe_audio(
        self,
        audio_path: str,
        language: str = "ja",
        enable_diarization: bool = True,
        min_speakers: int = 1,
        max_speakers: int = 10,
    ) -> Dict[str, Any]:
        """
        éŸ³å£°æ–‡å­—èµ·ã“ã—ï¼ˆWhisperX + è©±è€…åˆ†é›¢ï¼‰

        Args:
            audio_path: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            language: è¨€èªã‚³ãƒ¼ãƒ‰
            enable_diarization: è©±è€…åˆ†é›¢ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹
            min_speakers: æœ€å°è©±è€…æ•°
            max_speakers: æœ€å¤§è©±è€…æ•°

        Returns:
            æ–‡å­—èµ·ã“ã—çµæœ
        """
        import whisperx

        print(f"ğŸ™ï¸ éŸ³å£°æ–‡å­—èµ·ã“ã—é–‹å§‹: {audio_path}")

        try:
            # éŸ³å£°èª­ã¿è¾¼ã¿
            audio = whisperx.load_audio(audio_path)

            # WhisperXã§æ–‡å­—èµ·ã“ã—
            print("ğŸ“ WhisperXã§æ–‡å­—èµ·ã“ã—ä¸­...")
            result = self.whisperx_model.transcribe(
                audio,
                language=language,
                batch_size=16,
            )

            # å˜èªãƒ¬ãƒ™ãƒ«ã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ
            print("ğŸ”¤ å˜èªãƒ¬ãƒ™ãƒ«ã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆä¸­...")
            model_a, metadata = whisperx.load_align_model(
                language_code=language,
                device=self.device,
            )
            result = whisperx.align(
                result["segments"],
                model_a,
                metadata,
                audio,
                self.device,
                return_char_alignments=False,
            )

            # è©±è€…åˆ†é›¢
            speakers_result = None
            if enable_diarization and self.diarization_pipeline:
                print("ğŸ¤ è©±è€…åˆ†é›¢ä¸­...")
                try:
                    diarization = self.diarization_pipeline(
                        audio_path,
                        min_speakers=min_speakers,
                        max_speakers=max_speakers,
                    )

                    # è©±è€…æƒ…å ±ã‚’çµæœã«çµ±åˆ
                    result = whisperx.assign_word_speakers(diarization, result)
                    speakers_result = self._extract_speakers(result)
                    print(f"âœ… {len(set(speakers_result))} äººã®è©±è€…ã‚’æ¤œå‡º")
                except Exception as e:
                    print(f"âš ï¸ è©±è€…åˆ†é›¢ã‚¨ãƒ©ãƒ¼: {e}")

            # çµæœã®æ•´å½¢
            output = {
                "text": " ".join([seg["text"] for seg in result["segments"]]),
                "segments": result["segments"],
                "language": language,
                "speakers": speakers_result,
                "timestamp": datetime.now().isoformat(),
            }

            print("âœ… éŸ³å£°æ–‡å­—èµ·ã“ã—å®Œäº†")
            return output

        except Exception as e:
            print(f"âŒ éŸ³å£°æ–‡å­—èµ·ã“ã—ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    @modal.method()
    def analyze_image(
        self,
        image_data: bytes,
        prompt: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        ç”»åƒåˆ†æï¼ˆClaude Vision or GPT-4Vï¼‰

        Args:
            image_data: ç”»åƒãƒ‡ãƒ¼ã‚¿ï¼ˆãƒã‚¤ãƒˆåˆ—ï¼‰
            prompt: ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ

        Returns:
            ç”»åƒåˆ†æçµæœ
        """
        import base64

        print(f"ğŸ–¼ï¸ ç”»åƒåˆ†æé–‹å§‹ï¼ˆãƒ—ãƒ©ã‚¤ãƒãƒª: {self.primary_llm_provider}ï¼‰")

        # Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        image_base64 = base64.b64encode(image_data).decode("utf-8")

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        if not prompt:
            prompt = """
ã“ã®ç”»åƒã‚’è©³ã—ãåˆ†æã—ã¦ãã ã•ã„ï¼š
1. ä½•ãŒæ˜ ã£ã¦ã„ã‚‹ã‹ï¼ˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€äººç‰©ã€é¢¨æ™¯ãªã©ï¼‰
2. æŠ€è¡“çš„ãªè¦ç´ ï¼ˆã‚³ãƒ¼ãƒ‰ã€å›³ã€UIã€ãƒ›ãƒ¯ã‚¤ãƒˆãƒœãƒ¼ãƒ‰ãªã©ï¼‰
3. é‡è¦ãªæƒ…å ±ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã€æ•°å­—ã€è¨˜å·ãªã©ï¼‰
4. å…¨ä½“çš„ãªã‚·ãƒ¼ãƒ³ã®èª¬æ˜

JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š
{
  "description": "å…¨ä½“çš„ãªèª¬æ˜",
  "objects": ["æ¤œå‡ºã•ã‚ŒãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãƒªã‚¹ãƒˆ"],
  "text": "ç”»åƒå†…ã®ãƒ†ã‚­ã‚¹ãƒˆ",
  "scene": "ã‚·ãƒ¼ãƒ³ã®ç¨®é¡",
  "technical_elements": ["æŠ€è¡“çš„ãªè¦ç´ "]
}
"""

        try:
            # ãƒ—ãƒ©ã‚¤ãƒãƒªLLMã§è©¦è¡Œ
            result = self._analyze_image_with_provider(
                image_base64, 
                prompt, 
                self.primary_llm_provider
            )
            return result

        except Exception as e:
            print(f"âš ï¸ ãƒ—ãƒ©ã‚¤ãƒãƒªLLMï¼ˆ{self.primary_llm_provider}ï¼‰ã‚¨ãƒ©ãƒ¼: {e}")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãŒæœ‰åŠ¹ãªå ´åˆ
            if self.enable_llm_fallback:
                fallback_provider = "anthropic" if self.primary_llm_provider == "openai" else "openai"
                print(f"ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {fallback_provider}ã§å†è©¦è¡Œ")
                
                try:
                    result = self._analyze_image_with_provider(
                        image_base64, 
                        prompt, 
                        fallback_provider
                    )
                    return result
                except Exception as fallback_error:
                    print(f"âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯LLMã‚‚ã‚¨ãƒ©ãƒ¼: {fallback_error}")
                    raise
            else:
                raise

    @modal.method()
    def detect_scene_changes(
        self,
        frames: List[bytes],
        threshold: float = 0.3,
    ) -> List[int]:
        """
        ã‚·ãƒ¼ãƒ³å¤‰åŒ–æ¤œå‡º

        Args:
            frames: ãƒ•ãƒ¬ãƒ¼ãƒ ç”»åƒã®ãƒªã‚¹ãƒˆ
            threshold: å¤‰åŒ–æ¤œå‡ºã®é–¾å€¤

        Returns:
            ã‚·ãƒ¼ãƒ³å¤‰åŒ–ãŒæ¤œå‡ºã•ã‚ŒãŸãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        """
        import cv2
        import numpy as np

        print(f"ğŸ¬ ã‚·ãƒ¼ãƒ³å¤‰åŒ–æ¤œå‡ºé–‹å§‹ï¼ˆ{len(frames)} ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰")

        try:
            changes = []
            prev_frame = None

            for i, frame_data in enumerate(frames):
                # ãƒã‚¤ãƒˆåˆ—ã‹ã‚‰ç”»åƒã«å¤‰æ›
                nparr = np.frombuffer(frame_data, np.uint8)
                frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

                if prev_frame is not None:
                    # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ æ¯”è¼ƒ
                    hist1 = cv2.calcHist([prev_frame], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])
                    hist2 = cv2.calcHist([frame], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])

                    cv2.normalize(hist1, hist1)
                    cv2.normalize(hist2, hist2)

                    # é¡ä¼¼åº¦è¨ˆç®—
                    similarity = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)

                    # é–¾å€¤ä»¥ä¸‹ãªã‚‰å¤‰åŒ–ã¨ã¿ãªã™
                    if similarity < (1.0 - threshold):
                        changes.append(i)
                        print(f"  ğŸ“ ãƒ•ãƒ¬ãƒ¼ãƒ  {i}: å¤‰åŒ–æ¤œå‡ºï¼ˆé¡ä¼¼åº¦: {similarity:.3f}ï¼‰")

                prev_frame = frame

            print(f"âœ… {len(changes)} ç®‡æ‰€ã®ã‚·ãƒ¼ãƒ³å¤‰åŒ–ã‚’æ¤œå‡º")
            return changes

        except Exception as e:
            print(f"âŒ ã‚·ãƒ¼ãƒ³å¤‰åŒ–æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            raise

    @modal.method()
    def generate_specification(
        self,
        context: Dict[str, Any],
    ) -> Dict[str, Any]:
        """
        ä»•æ§˜æ›¸ç”Ÿæˆ

        Args:
            context: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ï¼ˆæ–‡å­—èµ·ã“ã—ã€ç”»åƒãªã©ï¼‰

        Returns:
            ç”Ÿæˆã•ã‚ŒãŸä»•æ§˜æ›¸
        """
        print(f"ğŸ“„ ä»•æ§˜æ›¸ç”Ÿæˆé–‹å§‹ï¼ˆãƒ—ãƒ©ã‚¤ãƒãƒª: {self.primary_llm_provider}ï¼‰")

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰
        prompt = self._build_specification_prompt(context)

        try:
            # ãƒ—ãƒ©ã‚¤ãƒãƒªLLMã§è©¦è¡Œ
            result = self._generate_text_with_provider(
                prompt,
                self.primary_llm_provider,
                max_tokens=4096
            )
            
            specification_text = result["content"]
            
            output = {
                "title": self._extract_title(specification_text),
                "content": specification_text,
                "model": result["model"],
                "timestamp": datetime.now().isoformat(),
            }

            print("âœ… ä»•æ§˜æ›¸ç”Ÿæˆå®Œäº†")
            return output

        except Exception as e:
            print(f"âš ï¸ ãƒ—ãƒ©ã‚¤ãƒãƒªLLMï¼ˆ{self.primary_llm_provider}ï¼‰ã‚¨ãƒ©ãƒ¼: {e}")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãŒæœ‰åŠ¹ãªå ´åˆ
            if self.enable_llm_fallback:
                fallback_provider = "anthropic" if self.primary_llm_provider == "openai" else "openai"
                print(f"ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {fallback_provider}ã§å†è©¦è¡Œ")
                
                try:
                    result = self._generate_text_with_provider(
                        prompt,
                        fallback_provider,
                        max_tokens=4096
                    )
                    
                    specification_text = result["content"]
                    
                    output = {
                        "title": self._extract_title(specification_text),
                        "content": specification_text,
                        "model": result["model"],
                        "timestamp": datetime.now().isoformat(),
                    }

                    print("âœ… ä»•æ§˜æ›¸ç”Ÿæˆå®Œäº†ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰")
                    return output
                    
                except Exception as fallback_error:
                    print(f"âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯LLMã‚‚ã‚¨ãƒ©ãƒ¼: {fallback_error}")
                    raise
            else:
                raise

    # ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰

    def _extract_speakers(self, result: Dict) -> List[str]:
        """è©±è€…ãƒªã‚¹ãƒˆã‚’æŠ½å‡º"""
        speakers = set()
        for segment in result.get("segments", []):
            if "speaker" in segment:
                speakers.add(segment["speaker"])
        return sorted(list(speakers))

    def _build_specification_prompt(self, context: Dict[str, Any]) -> str:
        """ä»•æ§˜æ›¸ç”Ÿæˆç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰"""
        transcriptions = context.get("transcriptions", [])
        photos = context.get("photos", [])

        prompt = """
ä»¥ä¸‹ã®ä¼šè­°ãƒ»ä½œæ¥­ã®è¨˜éŒ²ã‹ã‚‰ã€æŠ€è¡“ä»•æ§˜æ›¸ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

# æ–‡å­—èµ·ã“ã—
"""
        for trans in transcriptions[-20:]:  # æœ€æ–°20ä»¶
            speaker = trans.get("speaker", "ä¸æ˜")
            text = trans.get("text", "")
            prompt += f"\n[{speaker}] {text}"

        prompt += """

# è¦æ±‚äº‹é …
- æ˜ç¢ºã§æ§‹é€ åŒ–ã•ã‚ŒãŸä»•æ§˜æ›¸ã‚’ä½œæˆ
- è¦ä»¶ã€æŠ€è¡“è©³ç´°ã€å®Ÿè£…æ‰‹é †ã‚’å«ã‚ã‚‹
- Markdownå½¢å¼ã§å‡ºåŠ›

# å‡ºåŠ›å½¢å¼
## ã‚¿ã‚¤ãƒˆãƒ«
## æ¦‚è¦
## è¦ä»¶
### æ©Ÿèƒ½è¦ä»¶
### éæ©Ÿèƒ½è¦ä»¶
## æŠ€è¡“è©³ç´°
## å®Ÿè£…æ‰‹é †
## å‚™è€ƒ
"""
        return prompt

    def _extract_title(self, text: str) -> str:
        """ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º"""
        lines = text.split("\n")
        for line in lines:
            if line.startswith("# "):
                return line.replace("# ", "").strip()
        return "ä»•æ§˜æ›¸"

    def _analyze_image_with_provider(
        self,
        image_base64: str,
        prompt: str,
        provider: str,
    ) -> Dict[str, Any]:
        """
        æŒ‡å®šã•ã‚ŒãŸãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§ç”»åƒåˆ†æã‚’å®Ÿè¡Œ

        Args:
            image_base64: Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒ
            prompt: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            provider: 'openai' ã¾ãŸã¯ 'anthropic'

        Returns:
            ç”»åƒåˆ†æçµæœ
        """
        if provider == "openai":
            from openai import OpenAI
            
            print(f"  ğŸ¤– OpenAI ({self.openai_model})ã§ç”»åƒåˆ†æä¸­...")
            client = OpenAI(api_key=self.openai_api_key)
            
            response = client.chat.completions.create(
                model=self.openai_model,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": prompt
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{image_base64}"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=1024,
            )
            
            return {
                "analysis": response.choices[0].message.content,
                "model": self.openai_model,
                "timestamp": datetime.now().isoformat(),
            }
            
        elif provider == "anthropic":
            from anthropic import Anthropic
            
            print(f"  ğŸ¤– Anthropic ({self.anthropic_model})ã§ç”»åƒåˆ†æä¸­...")
            client = Anthropic(api_key=self.anthropic_api_key)
            
            response = client.messages.create(
                model=self.anthropic_model,
                max_tokens=1024,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "image",
                                "source": {
                                    "type": "base64",
                                    "media_type": "image/jpeg",
                                    "data": image_base64,
                                },
                            },
                            {"type": "text", "text": prompt},
                        ],
                    }
                ],
            )
            
            return {
                "analysis": response.content[0].text,
                "model": self.anthropic_model,
                "timestamp": datetime.now().isoformat(),
            }
        else:
            raise ValueError(f"æœªã‚µãƒãƒ¼ãƒˆã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {provider}")

    def _generate_text_with_provider(
        self,
        prompt: str,
        provider: str,
        max_tokens: int = 4096,
    ) -> Dict[str, Any]:
        """
        æŒ‡å®šã•ã‚ŒãŸãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’å®Ÿè¡Œ

        Args:
            prompt: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            provider: 'openai' ã¾ãŸã¯ 'anthropic'
            max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°

        Returns:
            ç”Ÿæˆçµæœ
        """
        if provider == "openai":
            from openai import OpenAI
            
            print(f"  ğŸ¤– OpenAI ({self.openai_model})ã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆä¸­...")
            client = OpenAI(api_key=self.openai_api_key)
            
            response = client.chat.completions.create(
                model=self.openai_model,
                messages=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                max_tokens=max_tokens,
            )
            
            return {
                "content": response.choices[0].message.content,
                "model": self.openai_model,
            }
            
        elif provider == "anthropic":
            from anthropic import Anthropic
            
            print(f"  ğŸ¤– Anthropic ({self.anthropic_model})ã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆä¸­...")
            client = Anthropic(api_key=self.anthropic_api_key)
            
            response = client.messages.create(
                model=self.anthropic_model,
                max_tokens=max_tokens,
                messages=[
                    {
                        "role": "user",
                        "content": prompt,
                    }
                ],
            )
            
            return {
                "content": response.content[0].text,
                "model": self.anthropic_model,
            }
        else:
            raise ValueError(f"æœªã‚µãƒãƒ¼ãƒˆã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {provider}")


# FastAPI Webã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.function(image=image, secrets=secrets)
@modal.asgi_app()
def fastapi_app():
    """
    FastAPI Webã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    """
    from fastapi import FastAPI, UploadFile, File, Form, Request
    from fastapi.responses import JSONResponse

    web_app = FastAPI(title="Realworld Agent GPU API")

    @web_app.get("/")
    async def root():
        return {
            "name": "Realworld Agent GPU Server",
            "version": "0.1.0",
            "status": "running",
        }

    @web_app.get("/health")
    async def health():
        return {"status": "healthy"}

    @web_app.post("/api/transcribe")
    async def transcribe(
        audio: UploadFile = File(...),
        language: str = Form("ja"),
        enable_diarization: bool = Form(True),
    ):
        """éŸ³å£°æ–‡å­—èµ·ã“ã—API"""
        try:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            temp_path = f"/tmp/{audio.filename}"
            with open(temp_path, "wb") as f:
                f.write(await audio.read())

            # GPUã‚¯ãƒ©ã‚¹ã‚’å‘¼ã³å‡ºã—
            gpu = RealworldAgentGPU()
            result = gpu.transcribe_audio.remote(
                temp_path,
                language=language,
                enable_diarization=enable_diarization,
            )

            return JSONResponse(content=result)

        except Exception as e:
            return JSONResponse(
                status_code=500,
                content={"error": str(e)},
            )

    @web_app.post("/api/analyze-image")
    async def analyze_image(image: UploadFile = File(...)):
        """ç”»åƒåˆ†æAPI"""
        try:
            image_data = await image.read()

            gpu = RealworldAgentGPU()
            result = gpu.analyze_image.remote(image_data)

            return JSONResponse(content=result)

        except Exception as e:
            return JSONResponse(
                status_code=500,
                content={"error": str(e)},
            )

    @web_app.post("/generate-spec")
    async def generate_specification(request: Request):
        """ä»•æ§˜æ›¸ç”ŸæˆAPI"""
        try:
            body = await request.json()
            context = body.get("context", {})

            gpu = RealworldAgentGPU()
            result = gpu.generate_specification.remote(context)

            return JSONResponse(content=result)

        except Exception as e:
            return JSONResponse(
                status_code=500,
                content={"error": str(e)},
            )

    return web_app

