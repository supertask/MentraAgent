# LLMプロバイダー切り替えとフォールバック

**ファイル**: `services/gpu-server/modal_app.py`, `services/api-server/src/config.ts`

## このプロジェクトの実装

**フォールバックロジック**（modal_app.py）:
```python
# プライマリで試行 → 失敗 → フォールバックで再試行
try:
    result = self._generate_text_with_provider(prompt, self.primary_llm_provider)
except Exception:
    if self.enable_llm_fallback:
        fallback_provider = "anthropic" if self.primary_llm_provider == "openai" else "openai"
        result = self._generate_text_with_provider(prompt, fallback_provider)
```

**環境変数**:
```env
PRIMARY_LLM_PROVIDER=openai          # openai or anthropic
ENABLE_LLM_FALLBACK=true             # 推奨: true
OPENAI_MODEL=gpt-4o
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
```

## よくあるミス

**❌ プロバイダーをハードコード**
```python
client = anthropic.Anthropic(...)  # 固定は避ける
```

**✅ 環境変数で制御**
```python
self._generate_text_with_provider(prompt, self.primary_llm_provider)
```

## Modal Secrets設定

```bash
modal secret create realworld-agent-secrets \
  PRIMARY_LLM_PROVIDER=openai \
  ENABLE_LLM_FALLBACK=true \
  OPENAI_API_KEY=$OPENAI_API_KEY \
  ANTHROPIC_API_KEY=$ANTHROPIC_API_KEY
```

