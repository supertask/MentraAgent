# リアル世界システム (Realworld Agent)

[Mentra Glass](https://mentraglass.com/)（GPU非搭載エッジデバイス）またはWebカメラのリアルタイム音声・カメラ機能とGPU搭載サーバー(Modal)を組み合わせ、現場やMeetingでの会話や視覚情報から自動的に仕様書を作成し、プログラムまで生成するシステムです。

## 🎯 現在の実装状況

### ✅ 実装済み機能

1. **入力デバイス**
   - Webカメラ入力（ブラウザ経由）
   - MentraOS統合（準備済み）
   - リアルタイム文字起こし（ブラウザ内蔵API）
   - 写真撮影機能

2. **処理基盤**
   - Fastify APIサーバー
   - PostgreSQL + Prisma
   - Redis キャッシュ
   - Modal GPUサーバー

3. **AI処理**
   - 仕様書生成（OpenAI/Anthropic対応）
   - LLMプロバイダー優先度設定＋フォールバック
   - 画像分析（準備済み）

4. **データ管理**
   - セッション管理
   - 文字起こし保存
   - 写真保存
   - 仕様書保存

5. **外部連携**
   - Slack統合
   - GitHub統合
   - Notion統合

### 🚧 今後の実装予定

1. **WhisperX統合**: 高精度文字起こしと単語レベルタイムスタンプ
2. **pyannote統合**: 話者分離（誰が何を言ったか）
3. **タイムスタンプ連携**: 音声と画像の時間同期
4. **Multi-RAGシステム**: 社内ドキュメント検索・統合

音声で重要な場所とそれに該当しそうなフレームや、特徴のあるフレーム（大きく変わるフレームなど）などを入力としています。

## 💰 コスト最適化

システムは以下のコスト最適化を実施済みです：

1. **Modal GPUサーバー**: keep_warm無効化（使った分だけ課金）
2. **GPU不使用**: 仕様書生成はCPUのみで動作
3. **LLMプロバイダー選択**: OpenAI/Anthropicを状況に応じて使い分け
4. **フォールバック機構**: プライマリが失敗しても自動的にセカンダリを使用

**月額コスト概算**: $10-135（使用量による）

## 🚀 クイックスタート

詳細は [SETUP.md](../SETUP.md) と [QUICKSTART_JA.md](../QUICKSTART_JA.md) を参照してください。

## 参考になりそうなシステム（ChatGPTのDeepSearchで検索したもの）

**Webカメラ→GPUサーバ**で動く試作に限れば、

- リアルタイム入出力の“土台”は **Pipecat**（音声・映像をストリーミングで捌き、ASR/LLM/合成音声や外部処理を配線しやすい）  
- 文字起こしは **Whisper/Whisper.cpp**、高精度タイムスタンプや後段処理の安定性が要るなら **WhisperX**  
- 複数話者の会議を前提とするなら **pyannote + Whisper 系**で**話者分離（ダイアライゼーション）**を付与  
- 「重要フレーム／大きく変わるフレーム」抽出は、まずは Pipecat の映像ストリームから受けたフレームを**シーンチェンジ検出＋キーフレーム抽出**（OpenCV 等）→必要なら **AI Video Summarizer**系の「スマートクリップ化」ロジックを流用  
- 仕様書／コード生成は、上記で刻んだ**話者付きテキスト＋キーフレーム**を条件に、RAG で外部ナレッジ（社内仕様・API・既存コード）へ引き当てる。研究寄りだが設計の参考に **Multi-RAG**（マルチモーダルRAG）をコンセプトとして採用  
- 将来的に**音声・映像を“ひとつのモデル”で**扱いたいなら **InteractiveOmni** を検証ブランチで

という組み合わせが一番“今すぐ作れて拡張しやすい”です。下に役割ごとに採用理由と参照をまとめます。

---

## 1) リアルタイム土台（配線・オーケストレーション）
**Pipecat**：音声・映像ストリームと ASR/LLM/音声合成などを**リアルタイムに繋ぐフレームワーク**。Web/モバイル/組込み向けのクライアントや React 向け UI キット、対話フロー編集の “pipecat-flows” もあり、**低遅延の双方向会話**に強いです。Mentra 置き換え時も**入力を差し替えるだけ**で済む構成にしやすい。 

**こう使う**  
- Edge（現状はPCのWebカメラ/マイク、将来はMentra）→ WebRTC → Pipecat（サーバ）  
- Pipecat ノードで VAD→ASR→キューイング→RAG→要約/仕様書/コード生成→TTS/通知まで１本のストリームに

---

## 2) 文字起こし（高精度タイムスタンプ付き）
**WhisperX**：Whisper の出力に**強制アラインメント**をかけて**単語レベルのタイムスタンプ**と**改善された区間境界**を付与。**VAD**や（最近は）**ダイアライゼーション統合**も進んでおり、後段の「重要箇所に対応する映像フレーム」を**正確に結びやすい**。GPU サーバでのバッチ/ストリーム活用例も多いです。 

**補助**：CPUや軽量環境での動作を優先する場面は **whisper.cpp** を使い、のちに WhisperX へ切替でもOK。（WhisperX は Alignment/Diarization 部分の GPU 利用率が相対的に伸びにくいという実測報告もあり、パイプライン分割が吉） 

---

## 3) 複数話者の会議前提なら：話者分離
**pyannote + Whisper 系**：Whisper 出力に **pyannote.audio** の話者分離を重ねて、**誰がいつ何を言ったか**を付与。CLI で Whisper 互換の使い勝手を目指した実装や、word-by-word のアライン戦略の代替実装も公開されており、要件に応じて選べます。 

---

## 4) 重要箇所抽出（音声×映像の同期）
**まずはルールベース＋軽ML**から：  
- 音声側は **WhisperX の単語タイムスタンプ**＋**キーワード・固有表現**＋**話者（役職/プロジェクト担当）**で“重要発話”を抽出。  
- 映像側は **シーンチェンジ検出**（フレーム差分・ヒストグラム・SSIM 等）＋**キーフレーム保存**。  
- **時間で突き合わせ**て、「重要発話に最も近い映像フレーム」をタグ付け。

「自動ハイライト／スマートクリップ」まで踏み込むなら、**AI Video Summarizer**の設計（WhisperXで起こして LLMでサマリ化、かつ**重要クリップ自動生成**）が流用しやすいです。まずは**バッチ**で検証→要件が固まったら **Pipecat ストリームに組込み**でリアルタイム化、が安全。 

---

## 5) 仕様書／コード生成（RAGで“社内知識”に繋ぐ）
- **Multi-RAG**：動画・音声・テキストと**外部知識**を突き合わせる**マルチモーダルRAG**の設計。**情報量の多い現場**でのアシストを想定しており、今回の「会議や現場の映像＋音声→仕様書/プログラム」要件に**そのまま合う思想**です。実装は既存の RAG フレームワーク（LlamaIndex/LangChain 等）で置換しつつ、**設計原理**を拝借するのが現実的。 

- **軽量オムニモデル路線（将来拡張）**：**InteractiveOmni** は**音声・画像・動画・テキストの入出力を単一モデル**に統合した研究プロジェクト。**4B/8B 級**でリアルタイム指向。現時点では**研究・検証ブランチ**として「会話ストリーム制御や評価」に使い、プロダクションは上記モジュール分離（Pipecat+ASR+RAG）を推奨。 

---

## 6) 付帯ユースケース（議事要約・Slack連携など）
- **Meetily（meeting-minutes）**：完全ローカル動作の**ライブ議事録**。プライバシー要件が強い現場検証に向く。Pipecat 置き換え前の**要約体験の早期テスト**に。   
- **Meeting Concluder**：録音→要約→**Slack 投稿**までの軽量パイプライン。**運用オペ**（通知・配信）を先に固めるのに使える。 

---

## 実装済みアーキテクチャ（現在のバージョン）

### 基本構成
1) **Edge**：Webカメラ/マイク（Mentra Glass対応準備済み）  
2) **Ingress**：WebSocket → **Fastify API Server**  
3) **音声系**：ブラウザ内蔵音声認識 → リアルタイム文字起こし（※WhisperX統合予定）  
4) **映像系**：写真撮影（手動/自動）→ ローカルストレージ保存  
5) **処理**：**Modal GPUサーバー**でLLM処理（OpenAI/Anthropic）  
6) **生成**：文字起こし＋写真を統合して**仕様書自動生成**  
7) **配信**：データベース保存 + 外部連携（Slack/Notion/GitHub）準備済み

## 推奨アーキテクチャ（今後の拡張）
1) **Edge**：Webカメラ/マイク（後に Mentra に差替）  
2) **Ingress**：WebRTC → **Pipecat** （検討中）  
3) **音声系**：VAD → **Whisper/WhisperX** → **pyannote**で話者付与（再有効化予定）  
4) **映像系**：シーンチェンジ検出→**キーフレーム抽出**（重要発話の近傍にタグ）  
5) **知識結合**：**RAG**（設計基準は **Multi-RAG**）で社内ドキュメント・API仕様・コードベースに引き当て  
6) **生成**：仕様書テンプレへの**構造化出力**＋**コード生成**（安全のため"PR下書き"や"サンドボックス実行"）  
7) **配信**：要約・仕様書・PRリンク・クリップを **Slack/Notion/GitHub** へ自動配信（Concluderや既存Botの実装を参考）

---

## 「まずGitHubで似た人を探す」ためのキーワード
- `Pipecat` / `real-time multimodal agent` / `WebRTC voice agent`（**Pipecat 事例やflows**）   
- `WhisperX diarization alignment meeting`（**WhisperX＋話者分離**の実戦例）   
- `pyannote whisper diarization realtime`（**会議録×話者**の常連）   
- `video summarizer smart clips WhisperX`（**スマートクリップ**系）   
- `multimodal RAG video audio text`（**Multi-RAG**思想） 

---

### まとめ

#### 現在の実装
- ✅ **基本動作**：Webカメラ × Fastify × Modal で仕様書生成が完全動作
- ✅ **データ管理**：PostgreSQL + Prisma で文字起こし・写真・仕様書を永続化
- ✅ **LLM連携**：OpenAI/Anthropic 両対応、優先度設定＋フォールバック
- ✅ **コスト最適化**：keep_warm無効化、使った分だけ課金

#### 次のステップ
- 🔄 **WhisperX統合**：高精度文字起こしと単語レベルタイムスタンプ
- 🔄 **pyannote統合**：話者分離（誰が何を言ったか）
- 🔄 **タイムスタンプ連携**：音声と画像の時間同期
- 🔄 **Multi-RAG実装**：社内ドキュメント検索・統合

#### 将来の拡張（検討中）
- **今すぐ動かす核**：Pipecat × WhisperX（＋pyannote）  
- **ハイライト生成**：AI Video Summarizer の"スマートクリップ"設計を流用  
- **仕様書/コード**：RAG 基盤を Multi-RAG の設計思想で  
- **将来拡張**（Mentra常用＆単一モデル志向）：InteractiveOmni を別ブランチで評価

この順で積むと、**Mentra なしWebカメラ実装 → Mentra差し替え → 単一モデル統合検証**まで綺麗に道が繋がります。

---

**最終更新**: 2025-10-25  
**実装進捗**: 約90%  
**詳細**: [PROGRESS.md](../PROGRESS.md) を参照



